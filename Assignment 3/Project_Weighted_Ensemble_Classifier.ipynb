{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Weighted Ensemble Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faeFR_aDU7u3",
        "colab_type": "text"
      },
      "source": [
        "**Implementation of a Weighted Ensemble Classifier**\n",
        "\n",
        "```\n",
        "Implementation of a Weighted Ensemble Classifier.\n",
        "The implementation follows the techniques described in\n",
        "\"Mining Concept-Drifting Data Streams using Ensemble Classifiers\", by\n",
        "Haixun Wang, Wei Fan, Philip S. Yu, Jiawei Han\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OxuV7CHUjpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Useful Imports\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import operator\n",
        "import copy as cp\n",
        "import sortedcontainers as sc\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5JER1L7JLZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Class WeightedEnsembleClassifier\n",
        "\n",
        "class WeightedEnsembleClassifier:\n",
        "                                                            #k: maximum number of classifiers in the ensemble\n",
        "                                                            #S: chunk size\n",
        "                                                            #base learner: base astimator\n",
        "                                                            #CV: number of folds to compute the score of a newly added classifier\n",
        "\n",
        "    class WeightedClassifier:                             #An inner class to control weights and additional information of a base learner in the ensemble\n",
        "     #Create a new weighted classifier\n",
        "         \n",
        "        def __init__(self, clf, weight, chunk_labels):                   \n",
        "            self.clf = clf                                  \n",
        "            self.weight = weight                            # the weight associated to this classifier\n",
        "            self.chunk_labels = chunk_labels                # the unique labels of the data chunk the classifier(clf) is trained on\n",
        "\n",
        "        def __lt__(self, other):                            # Compares an object of this class to the other by means of the weight\n",
        "            return self.weight < other.weight               # for sorting the classifier in sorted list and returns true (smaller weight than other weight)\n",
        "\n",
        "    def __init__(self, K=10, base_learner=DecisionTreeClassifier(), S=200, cv=5):           #new ensemble\n",
        "                                                                                            #K: the maximum number of classifier in this ensemble\n",
        "                                                                                            #base_learner: the base learner, other classifiers will be a deep-copy of the base learner\n",
        "                                                                                            #S: the chunk size\n",
        "                                                                                            #cv: the number of folds for cross-validation\n",
        "        self.K = K                                           # top K classifiers\n",
        "        self.base_learner = base_learner                     # base learner\n",
        "        self.models = sc.SortedList()                        # a sorted list if classifiers        \n",
        "        self.cv = cv                                          # cross validation fold\n",
        "                                                           \n",
        "        # chunk-related information\n",
        "          \n",
        "        self.S = S                          # chunk size\n",
        "        self.p = -1                         # chunk pointer\n",
        "        self.X_chunk = None\n",
        "        self.y_chunk = None\n",
        "\n",
        "    def partial_fit(self, X, y=None, classes=None, weight=None):                     #Updates the ensemble when a new data chunk arrives\n",
        "                                                                                    #X: the training data\n",
        "                                                                                     #y: the training labels\n",
        "                                                                                      #classes:contains all possible labels\n",
        "                                                                                      #weight: array-like, instance weight, uniform weights are assumed if not provided\n",
        "        \n",
        "        N, D = X.shape\n",
        "\n",
        "        # initializes when the ensemble is first called\n",
        "        if self.p == -1:\n",
        "            self.X_chunk = np.zeros((self.S, D))\n",
        "            self.y_chunk = np.zeros(self.S)\n",
        "            self.p = 0\n",
        "\n",
        "        # fill up the data chunk\n",
        "        for i, x in enumerate(X):\n",
        "            self.X_chunk[self.p] = X[i]\n",
        "            self.y_chunk[self.p] = y[i]\n",
        "            self.p += 1\n",
        "\n",
        "            if self.p == self.S:\n",
        "                self.p = 0                                                # reset the pointer (S is chunk size)\n",
        "\n",
        "                                                                          # retrieve the classes and class count\n",
        "                if classes is None:\n",
        "                    classes, class_count = np.unique(self.y_chunk, return_counts=True)\n",
        "                else:\n",
        "                    _, class_count = np.unique(self.y_chunk, return_counts=True)\n",
        "                C_new = cp.deepcopy(self.base_learner)                    # 1: train classifier C' from X by creating a deep copy from the base learner\n",
        "                try:\n",
        "                    C_new.fit(self.X_chunk, self.y_chunk)\n",
        "                except NotImplementedError:\n",
        "                    C_new.partial_fit(self.X_chunk, self.y_chunk, classes, weight)\n",
        "                \n",
        "              # compute the baseline error rate given by a random classifier\n",
        "                baseline_score = self.compute_random_baseline(classes)\n",
        "\n",
        "              # compute the weight of C', may do cross-validation if cv is not None\n",
        "                clf_new = self.WeightedClassifier(clf=C_new, weight=0, chunk_labels=classes)\n",
        "                clf_new.weight = self.compute_weight(model=clf_new, random_score=baseline_score, cv=self.cv)\n",
        "\n",
        "                for model in self.models:                                 # 4: update the weights of each classifier in the ensemble\n",
        "                    model.weights = self.compute_weight(model=model, random_score=baseline_score, cv=None)\n",
        "\n",
        "                if len(self.models) < self.K:                               # 5: C <- top K weighted classifiers in C U { C' }\n",
        "                    self.models.add(value=clf_new)\n",
        "                else:\n",
        "                    if clf_new.weight > 0 and clf_new.weight > self.models[0].weight:\n",
        "                        self.models.pop(0)\n",
        "                        self.models.add(value=clf_new)\n",
        "              \n",
        "        return self\n",
        "        pass  \n",
        "\n",
        "    def predict(self, X):                                                 #Predicts the labels of X in a general classification setting\n",
        "                                                                          # The prediction is done via normalized weighted voting (choosing the maximum)\n",
        "        N, D = X.shape                                                    #X: the unseen data to give predictions\n",
        "                                                                          #return: a list of shape (n-samples,) containing predictions\n",
        "        # List with size X.shape[0] and each value is a dict too,\n",
        "        # Ex: [{0:0.2, 1:0.7}, {1:0.3, 2:0.5}]\n",
        "        list_label_instance = []\n",
        "\n",
        "        # use sum_weights for normalization\n",
        "        sum_weights = np.sum([clf.weight for clf in self.models])\n",
        "\n",
        "        # For each classifier in self.models, predict the labels for X\n",
        "        for model in self.models:\n",
        "            clf = model.clf\n",
        "            pred = clf.predict(X)\n",
        "            weight = model.weight\n",
        "            for i, label in enumerate(pred.tolist()):\n",
        "                if i == len(list_label_instance):  # maintain the dictionary\n",
        "                    list_label_instance.append({label: weight / sum_weights})\n",
        "                else:\n",
        "                    try:\n",
        "                        list_label_instance[i][label] += weight / sum_weights\n",
        "                    except KeyError:\n",
        "                        list_label_instance[i][label] = weight / sum_weights\n",
        "\n",
        "        predict_weighted_voting = np.zeros(N)\n",
        "        for i, dic in enumerate(list_label_instance):\n",
        "            # return the key of max value in a dict\n",
        "            max_value = max(dic.items(), key=operator.itemgetter(1))[0]\n",
        "            predict_weighted_voting[i] = max_value\n",
        "\n",
        "        return predict_weighted_voting\n",
        "\n",
        "    def compute_score(self, model, X, y):                         #This compute the mean square error of a classifier. This code needs to take into account the fact that a classifier C trained on a\n",
        "                                                                  #previous data chunk may not have seen all the labels that appear in a new chunk\n",
        "                                                                  #(e.g. C is trained with only labels [1, 2] but the new chunk contains labels [1, 2, 3, 4, 5]\n",
        "                                                                  #X: data of the new chunk;y: labels of the new chunk;return: the mean square error MSE_i\n",
        "        N = len(y)\n",
        "        labels = model.chunk_labels\n",
        "        probabs = model.clf.predict_proba(X)\n",
        "        sum_error = 0\n",
        "        for i, c in enumerate(y):\n",
        "            # if the label in y is unseen when training, skip it, don't include it in the error\n",
        "            if c in labels:\n",
        "                index_label_c = np.where(labels == c)[0][0]  # find the index of this label c in probabs[i]\n",
        "                probab_ic = probabs[i][index_label_c]\n",
        "                sum_error += (1 - probab_ic) ** 2\n",
        "            else:\n",
        "                sum_error += 1\n",
        "        return sum_error / N\n",
        "\n",
        "    def compute_score_crossvalidation(self, model, cv):                      # Computes the score of interests, using cross-validation         \n",
        "                                                                            #model: the model to compute the score on\n",
        "                                                                           #cv: the number of folds, if None, the score is computed directly on the entire data chunk,\n",
        "        \n",
        "        if cv is not None and type(cv) is int:\n",
        "            # here I create a copy because here I don't want to \"modify\" an already trained model\n",
        "            copy_model = cp.deepcopy(model)\n",
        "            score = 0\n",
        "            sf = StratifiedKFold(n_splits=self.cv, shuffle=True, random_state=0)\n",
        "            for train_idx, test_idx in sf.split(X=self.X_chunk, y=self.y_chunk):\n",
        "                X_train, y_train = self.X_chunk[train_idx], self.y_chunk[train_idx]\n",
        "                X_test, y_test = self.X_chunk[test_idx], self.y_chunk[test_idx]\n",
        "                try:\n",
        "                    copy_model.clf.fit(X_train, y_train)\n",
        "                except NotImplementedError:\n",
        "                    copy_model.clf.partial_fit(X_train, y_train, copy_model.chunk_labels, None)\n",
        "                score += self.compute_score(model=copy_model, X=X_test, y=y_test) / self.cv\n",
        "        else:\n",
        "            # compute the score on the entire data chunk\n",
        "            score = self.compute_score(X=self.X_chunk, y=self.y_chunk, model=model)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def compute_weight(self, model, random_score, cv=None):                       #Compute the weight of a classifier given the random score (calculated on a random learner).\n",
        "                       \n",
        "        # compute MSE, with cross-validation or not\n",
        "        score = self.compute_score_crossvalidation(model=model, cv=cv)\n",
        "\n",
        "        # w = MSE_r = MSE_i\n",
        "        return random_score - score\n",
        "\n",
        "    def compute_random_baseline(self, classes):                                   #for the random baseline score(score produced by a random classifier)\n",
        "                                                                                   # The random score is MSE_r\n",
        "        # L = len(np.unique(classes))\n",
        "        # MSE_r = L * (1 / L) * (1 - 1 / L) ** 2\n",
        "        #base on the class distribution of the data --> count the number of labels\n",
        "        _, class_count = np.unique(classes, return_counts=True)\n",
        "        class_dist = [class_count[i] / self.S for i, c in enumerate(classes)]\n",
        "        mse_r = np.sum([class_dist[i] * ((1 - class_dist[i]) ** 2) for i, c in enumerate(classes)])\n",
        "        return mse_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3x47E0Laweh",
        "colab_type": "code",
        "outputId": "18a9a1a8-cc81-4258-f56a-eaede7108b7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "#Usefull imports \n",
        "!pip install -U scikit-multiflow\n",
        "from skmultiflow.evaluation.evaluate_prequential import EvaluatePrequential\n",
        "from skmultiflow.trees import HoeffdingTree\n",
        "from skmultiflow.data import FileStream\n",
        "import matplotlib as plt"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: scikit-multiflow in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (0.21.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: sortedcontainers>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (0.25.3)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-multiflow) (1.3.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->scikit-multiflow) (0.14.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.0->scikit-multiflow) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.0->scikit-multiflow) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (2.4.5)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->scikit-multiflow) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.21.0->scikit-multiflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->scikit-multiflow) (41.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMmq1CbggGRb",
        "colab_type": "code",
        "outputId": "a9d862a2-fdab-41ee-9dae-604033d37085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "#Evaluation of Weighted Ensemble Classifier \n",
        "\n",
        "# prepare the stream\n",
        "stream = FileStream('elec.csv',n_targets=1, target_idx=-1)\n",
        "stream.prepare_for_use()\n",
        "\n",
        "# instantiate a classifier\n",
        "clf = WeightedEnsembleClassifier(K=250, base_learner=HoeffdingTree(), cv=None)\n",
        "\n",
        "# setup the evaluator\n",
        "evaluator = EvaluatePrequential(pretrain_size=1000, max_samples=2000, show_plot=False,\n",
        "                                metrics=['accuracy'], batch_size=10)\n",
        "\n",
        "# 4. Run\n",
        "evaluator.evaluate(stream=stream, model=clf, model_names=[\"Weighted Ensemble Classifier \"])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prequential Evaluation\n",
            "Evaluating 1 target(s).\n",
            "Pre-training on 1000 sample(s).\n",
            "Evaluating...\n",
            " #################### [100%] [0.90s]\n",
            "Processed samples: 2000\n",
            "Mean performance:\n",
            "Weighted Ensemble Classifier  - Accuracy     : 0.7860\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.WeightedEnsembleClassifier at 0x7fbd7f608588>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An8AMcPrai-H",
        "colab_type": "text"
      },
      "source": [
        "Sources: greeks of greek, youtube, Github https://github.com/topics/data-stream"
      ]
    }
  ]
}